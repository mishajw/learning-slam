\documentclass{article}

\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{biblatex}
\usepackage[hidelinks]{hyperref}
\usepackage{algorithmicx}
\usepackage[noend]{algpseudocode}
\usepackage{xcolor}
\addbibresource{main.bib}

\title{Learning SLAM}
\author{Misha Wagner}

\newcommand{\todo}[1]{\textcolor{red}{TODO:\@ #1}}
\newcommand{\alignnote}[1]{\text{\footnotesize{#1}}}

\begin{document}

\maketitle
\tableofcontents
\newpage

\abstract{High-level notes on how SLAM works, with an accompanying Rust
implementation. Compiled from Cyrill Stachniss's course\cite{stachnisscourse}
and personal research.}

\section{What is SLAM?}

Simultaneous Localisation and Mapping, as it says on the tin, is a task where
the goal is to simultaneously predict:
\begin{itemize}
  \item A map of the environment (denoted as state within $x$)
  \item An agent's location in the map (denoted as state within $x$)
\end{itemize}
\dots given, at each timestep:
\begin{itemize}
  \item What the agent can see (denoted as $z$)
  \item How the agent is moving (denoted as $u$)
\end{itemize}

Therefore, the SLAM problem is to find a way to calculate this function:
\begin{equation}\label{slam}
  p(x \vert z, u)
\end{equation}
\dots for any given $x, z, u$, and thus find the $x$ that maximises this
function.

\subsection{Representations}

How do we represent each of the map, location, what's seen, and how the agent
is moving?

\todo{}

\subsection{Computation outline}

\section{Feature detection}

\section{SLAM algorithms}

These algorithms are the core of SLAM\@. They show how to construct
Equation~\ref{slam}, and how to optimize it.

\subsection{Bayes filters}

\subsubsection{Bayes filter definition}

Bayes filter is a transformation of the SLAM problem which allows us to solve
it recursively. This means we can solve when $t=0$, then $t=1$, all the way to
$t=T$.

We start with Equation~\ref{slam}, but make time explicit:
\begin{equation}
  bel(x_t) = p(x_t \vert z_{0:t}, u_{0:t})
\end{equation}

In English, this means that our belief in the state at time $t$ is the
probability of that state occurring at $t$, given everything we've seen and how
we've moved from the beginning of time to time $t$.

We then modify this equation:
\begin{align*}
  bel(x_t) &= p(x_t \vert z_{0:t}, u_{0:t}) \\
  &= \eta \cdot
    p(z_t \vert x_t, z_{0:t-1}, u_{0:t}) \cdot
    p(x_t \vert z_{0:t-1}, u_{0:t})
           & \alignnote{Bayes (Eq.~\ref{bayes})} \\
  &= \eta \cdot
    p(z_t \vert x_t, u_t) \cdot
    p(x_t \vert z_{0:t-1}, u_{0:t})
           & \alignnote{Markov (Eq.~\ref{markov})} \\
  &= \eta \cdot
    p(z_t \vert x_t) \cdot
    p(x_t \vert z_{0:t-1}, u_{0:t})
           & \alignnote{Independence} \\
  &= \eta \cdot
    p(z_t \vert x_t) \cdot
    \int_{x_{t-1}}
      p(x_t \vert x_{t-1}, z_{0:t-1}, u_{0:t}) \cdot
      p(x_{t-1} \vert z_{0:t-1}, u_{0:t})
           & \alignnote{Total prob. (Eq.~\ref{totalprobability})} \\
  &= \eta \cdot
    p(z_t \vert x_t) \cdot
    \int_{x_{t-1}}
      p(x_t \vert x_{t-1}, u_{t}) \cdot
      p(x_{t-1} \vert z_{0:t-1}, u_{0:t})
           & \alignnote{Markov (Eq.~\ref{markov})} \\
  &= \eta \cdot
    p(z_t \vert x_t) \cdot
    \int_{x_{t-1}}
      p(x_t \vert x_{t-1}, u_{t}) \cdot
      p(x_{t-1} \vert z_{0:t-1}, u_{0:t-1})
           & \alignnote{Independence} \\
  &= \eta \cdot
    p(z_t \vert x_t) \cdot
    \int_{x_{t-1}}
      p(x_t \vert x_{t-1}, u_{t}) \cdot bel(x_{t-1})
           & \alignnote{Recursion} \\
\end{align*}

The result of this transformation is an equation that makes intuitive sense.
Our belief that we're in state $x_t$ is the combination of:
\begin{itemize}
  \item The probability of us seeing what we're seeing, given we're in state
    $x_t$
  \item The probability that we've moved to this state, given where we thought
    we were previously
\end{itemize}

The new equation also allows us to solve $bel(x_T)$ recursively, and if we set
the origin of our coordinate system to our initial position, the base case is
trivial to solve.

We can then break the equation into two steps:
\begin{itemize}
  \item Prediction step:
    $\overline{bel}(x_t) = p(x_t \vert x_{t-1}, u_t)$
  \item Correction step:
    $\eta \cdot p(z_t \vert x_t) \cdot \overline{bel}(x_t)$
\end{itemize}
This results in two models we have to create:
\begin{itemize}
  \item Motion model: $p(x_t \vert x_{t-1}, u_t)$
  \item Observation model: $p(z_t \vert x_t)$
\end{itemize}

These models aren't defined by the Bayes filter, and are instead defined by the
other filters discussed in this section. Which one you choose to use, depends
on the nature of the problem. For example, some filters deal with non-linear
motion and observation models, and some are more performant than others.

\subsubsection{Kalman filter}

The Kalman filter assumes that the motion and observation models are linear,
and uses Gaussian distributions for noise. It is provably the best model for
linear Gaussian SLAM problems.

As motion and observation aren't often linear in reality, this model isn't
often used. However, it lays the foundation for other models that can
approximate non-linear functions.

We define $n$ as the number of elements in the state $x$, $k$ as the number of
observations returned at every step, and $l$ as the number of movement
parameters at every step.

The Kalman filter defines how we move as:
\begin{align*}
  x_t &= A_t x_{t-1} + B_t u_t + \epsilon_t \\
  \text{where }
  A_t &= \text{$(n \times n)$ matrix, mapping any natural movement in $x_{t-1}$
    to $x_t$} \\
  B_t &= \text{$(n \times l)$ matrix, describing how $u_t$ modifies $x_t$} \\
  \epsilon_t &= \text{$n$-vector additive noise on the state} \\
  \text{Resulting in the motion model:} \\
  p(x_t \vert x_{t-1}, u_t) &= \mathcal{N}(A_t x_{t-1} + B_t u_t, \epsilon_t)
\end{align*}

The Kalman filter defines what we observe as:
\begin{align*}
  z_t &= C_t x_t + \delta_t \\
  \text{where }
  C_t &= \text{$(k \times n)$ matrix, describing what $z_t$ should be
    observered from $x_t$} \\
  \delta_t &= \text{$k$-vector additive noise on the observations} \\
  \text{Resulting in the observation model:} \\
  p(z_t \vert x_t) &= \mathcal{N}(C_t x_t, \delta_t)
\end{align*}

Applying the two Kalman models results in this algorithm:
\begin{algorithmic}[1]
  \Procedure{KalmanFilter}{$\mu_{t-1}, \Sigma_{t-1}, z_t, u_t$}
    \State$\bar{\mu}_t = A_t \mu_{t-1} + B_t u_t$
      \Comment{Prediction step}
    \State$\bar{\Sigma}_t = A_t \Sigma_{t-1}A_t^T + \epsilon_t$
    \State$K_t =
      \bar{\Sigma}_t C_t^T{(C_t \bar{\Sigma}_t C_t^T + \delta_t)}^{-1}$
      \Comment{Correction step}
    \State$\mu_t = \bar{\mu}_t + K_t(z_t - C_t \bar{\mu_t})$
    \State$\Sigma_t = (I - K_t C_t)\hat{\Sigma}_t$
    \State\Return$\mu_t, \Sigma_t$
  \EndProcedure{}
\end{algorithmic}

\todo{Understand and explain $K_t$ stages in Kalman filter}

\subsubsection{Extended Kalman filter}

\subsubsection{Unscented Kalman filter}

\subsubsection{Extended information filter}

\subsubsection{Sparse extended information filter}

\subsection{Particle filters}

\section{Motion models}

\section{Miscellaneous}

\subsection{Bayes' rule}
\begin{equation}\label{bayes}
  p(a \vert b) = \frac{p(b \vert a) \cdot p(a)}{p(b)}
    = \eta \cdot p(b \vert a) \cdot p(a)
\end{equation}

\subsection{Markov assumption}
\begin{equation}\label{markov}
  p(x_t \vert x_{t-1}, x_{t-2}, \dots) = p(x_t \vert x_{t-1})
\end{equation}

\subsection{Law of total probability}
\begin{equation}\label{totalprobability}
  p(a \vert b) = \int_{c} p(a \vert b, c) \cdot p(c \vert b)
\end{equation}

\printbibliography{}

\end{document}
